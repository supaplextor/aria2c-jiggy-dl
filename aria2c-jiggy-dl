#!/usr/bin/perl
# git clone https://github.com/supaplextor/aria2c-jiggy-dl/ 
# http://geekofpassage.blogspot.com/
# (C) 2013 Scott Edwards <supadupa@gmail.com>
#
# jigdo cheating downloader.
# uses aria2
# hint: using "apt-get install balance" on 127.0.0.1:3128 to spread the load on all my squid servers.
# pass aria2 a list of mirrors (useful for 1M+ chunks)
# Note: uses 20 threads, to try and keep all (9 of my) proxy servers loaded with 2 urls concurrently at all times.

if (0 eq scalar(@ARGV)) {
	die "Pass at least one .jigdo file. All download files go here. Later, feed jigdo-lite build-dir directory for cache.";
}

use File::Basename qw(basename);
use List::Util qw/shuffle/;
use Data::Dumper;
use HTML::TokeParser::Simple;
use strict;
use warnings;

my $THREADS = 1;
my $site = "";
my $uri = "";
my @childs;
my $pkg;
my $pkgs;

sub dq_dot_jigdo
{
	my $self = shift;
	my $file = shift;
#	my $depth = 0;

	open (J,"-|",qw(gzip -dc),$file) or die "Cannot gzip -dc $file: $!";
	while(<J>) {
		chomp;
		last if $_ eq "[Parts]";
	}
	while(<J>) {
		next unless defined $_;
		chomp;
		if ($_ =~ m/^\[/) {
			last;
		}
		if (-1 eq index($_,":")) {
			next;
		}
		$pkg = basename $_;
		next if exists $pkgs->{$pkg};
		if ( (!-e $pkg) or (-s "$pkg.aria2") ) {
			my $l = "/".substr($_,1+index($_,":"));
			$pkgs->{$pkg}=$l;
		}
#		$depth++;
#		last if ($depth > 30);
	}
	while(<J>) {
		next unless defined $_;
		chomp;
		if (-1 eq index($_,":")) {
			next;
		}
		if ($_ =~ m!(http.?://snapshot.debian.org.*)!) {
			last;
		}
	}
	close J;
	my @pkgs = values %{$pkgs};
	$self->{"pkgs"} = \@pkgs;
}

sub mfork ($$&) {
	my ($count, $max, $code) = @_;
	foreach my $c (1 .. $count) {
		wait unless $c <= $max;
		die "Fork failed: $!\n" unless defined (my $pid = fork);
		exit $code -> ($c) unless $pid;
	}
	1 until -1 == wait;
}

# sub afork (\@$&) {
sub afork {
	my ($data, $max, $code) = @_;
	my $c = 0;
	foreach my $data (@$data) {
		wait unless ++ $c <= $max;
		die "Fork failed: $!\n" unless defined (my $pid = fork);
		exit $code -> ($data) unless $pid;
	}
	1 until -1 == wait;
}
=head1
#by Abigail of perlmonks.org
#Some times you have a need to fork of several children, but you want to
#limit the maximum number of children that are alive at one time. Here
#are two little subroutines that might help you, mfork and afork. They are very similar.
#They take three arguments,
#and differ in the first argument. For mfork, the first
#argument is a number, indicating how many children should be forked. For
#afork, the first argument is an array - a child will be
#forked for each array element. The second argument indicates the maximum
#number of children that may be alive at one time. The third argument is a
#code reference; this is the code that will be executed by the child. One
#argument will be given to this code fragment; for mfork it will be an increasing number,
#starting at one. Each next child gets the next number. For afork, the array element is
#passed. Note that this code will assume no other children will be spawned,
#and that $SIG {CHLD} hasn't been set to IGNORE.
=cut

sub my_aria2c {
	my $ec=-1;
	my $self = shift;
	my $urls = join(" ",@_);
	#$ec = system qq(aria2c -x5 -q -k1M --all-proxy="http://127.0.0.1:128" $urls);

	my $runme = qq(aria2c -d $self->{dotdeb_save_dir} -l pool/log-aria2.$$ -x5 -k10M $urls);
	$ec = system($runme);

	if (0 eq $ec) {
		print "$$ OK: $_[0] \n";
		unlink "pool/log-aria2.$$" or warn "Cannot unlink pool/log-aria2.$$: $!";;
		return $ec;
	} else {
		warn "aria2c returned exit code $ec and no idea why... See also pool/log-aria2.$$";
		die "I Quit!";
	}
	$ec = $ec >> 8;
	print "Error($ec): $_[0]\n";
	open L,"< pool/log-aria2.$$" or return $ec;
	print "$$ $_" while <L>;
	close L;
	return $ec;
}

#
# supaplex@wiseguy:~/usr/src/github-by-user/supaplextor/aria2c-jiggy-dl$ tail /proc/mounts 
# /dev/loop124 /home/supaplex/public_html/iso-images-FOSS/debian/build-dir/mounts/debian-10.0.0-i386-DVD-3 iso9660 ro,relatime,nojoliet,check=s,map=n,blocksize=2048,uid=1000,iocharset=utf8 0 0
# /dev/loop125 /home/supaplex/public_html/iso-images-FOSS/debian/build-dir/mounts/debian-13.0.0-amd64-DVD-8 iso9660 ro,relatime,nojoliet,check=s,map=n,blocksize=2048,uid=1000,iocharset=utf8 0 0
# /dev/loop126 /home/supaplex/public_html/iso-images-FOSS/debian/build-dir/mounts/debian-12.11.0-amd64-DVD-1 iso9660 ro,relatime,nojoliet,check=s,map=n,blocksize=2048,uid=1000,iocharset=utf8 0 0
# /dev/loop127 /home/supaplex/public_html/iso-images-FOSS/debian/build-dir/mounts/debian-mac-13.2.0-amd64-netinst iso9660 ro,relatime,nojoliet,check=s,map=n,blocksize=2048,uid=1000,iocharset=utf8 0 0
# /dev/loop128 /home/supaplex/public_html/iso-images-FOSS/debian/build-dir/mounts/debian-9.11.0-i386-netinst iso9660 ro,relatime,nojoliet,check=s,map=n,blocksize=2048,uid=1000,iocharset=utf8 0 0
# /dev/loop129 /home/supaplex/public_html/iso-images-FOSS/debian/build-dir/mounts/debian-edu-13.0.0-amd64-netinst iso9660 ro,relatime,nojoliet,check=s,map=n,blocksize=2048,uid=1000,iocharset=utf8 0 0
# 

sub mountpoints {
	my $self = shift;
	if (open my $fh, '<', '/proc/mounts') {
		while(<$fh>) {
			chomp;
			next unless $_;
			if ($_ =~ m/\/dev\/(sr|loop)[0-9]+ ([^ ]+) /) { # pro tip: skip /snap/ mounts, include all sr0+ mounts.
				my $destination = $2;
				next if $destination =~ m/^\/snap\//;
				push @{ $self->{dir_caches} }, $destination;
			} else {
				next;
			}
		}
	} else {
		die "Cannot open /proc/mounts for reading: $!";
	}
}

my @SITES;
sub mirrors {
	if (scalar(@SITES) > 0) {
		return
	}
	my $parser = HTML::TokeParser::Simple->new("list");
	my $tag;
	while ( $tag = $parser->get_tag('a') ) {
		if ( my $href = $tag->get_attr('href') ) {
			if ($href =~ m/\/debian/) {
				push(@SITES,$href)
			}
		}
	}
	return
}

sub leech {
	my $url = shift;
	my $self = {};
	return if ($url =~ m/snapshot.debian.org/);

	@{ $self->{dir_branches} }	= split(/\//,$url);
	$self->{dotdeb_basename}	= pop(@{ $self->{dir_branches} });
	shift @{ $self->{dir_branches} };

=head1
$VAR1 = {
          'dir_branches' => [
                              'pool',
                              'main',
                              'h',
                              'haskell-reflection'
                            ],
          'dotdeb_basename' => 'libghc-reflection-doc_2.1.8-2_all.deb'
        };
=cut

# build-dir/pool/...
	$self->{dotdeb_dirname}		= join("/" => "build-dir", @{ $self->{dir_branches} });
	-d $self->{dotdeb_dirname}	||	system("mkdir -vp ".quotemeta($self->{dotdeb_dirname}));

	my $save_to	=	$self->{dotdeb_dirname}. "/" . $self->{dotdeb_basename};
	if (-s $save_to) {
		warn "Already have $save_to";
		return 0;
	}

	$self->{dotdeb_dirname}		= join("/" => "debian", @{ $self->{dir_branches} });
	my $x = $self->{dotdeb_dirname} ."/" . $self->{dotdeb_basename};

	warn "Searching for $x in mounted dirs";
	for (@{ $self->{dir_caches}}) {
		if (-s $x) { 
			warn "Found $x in $_";
			my $ec;
			$ec = system qq(mkdir -p ).quotemeta($self->{dotdeb_dirname});
			warn $ec if $ec;
			$ec = system qq(rsync -vP ).quotemeta($x)." ".quotemeta($save_to);
			if ($ec) {
				warn $ec;
				unlink($save_to);
			};
			return 0 if -s $save_to;
		}
	}
	my @urls;
	for (shuffle(@SITES)) {
		push @urls,$_.$url;
	}
    $self->{urls_for_aria2c} = \@urls;
	$self->{dotdeb_save_dir}	=	$self->{dotdeb_dirname};
	my $ec = my_aria2c($self,@urls); 
	return $ec;
}

# main

my $self = {};
my @mountpoints = mountpoints($self);

my $ec=-1;
if (!-s "list") {
	$ec = system qq(aria2c https://www.debian.org/mirror/list );
	if (0 ne $ec) {
		die ("aria2c exit code $ec.");
	}
}
mirrors();
if (!-e "build-dir") {
	mkdir("build-dir");
}
if (!-e "pool") { # logs
	mkdir("pool");
}
for (@ARGV) {
	dq_dot_jigdo($self,$_);
#	warn Dumper($self);
	afork ($self->{"pkgs"},$THREADS,\&leech);
}
